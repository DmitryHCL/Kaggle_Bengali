{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv('C:/Users/HardcoreLoli/Desktop/!Deep/input/train.csv')\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "img_size = 128\n",
    "def read_data(nf):\n",
    "    nf=int(nf)\n",
    "    train_df = pd.read_feather(f'C:/Users/HardcoreLoli/Desktop/!Deep/input/train_image_data_{nf}.feather')\n",
    "    return train_df\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "def resize(df, img_size = 128, pad=16):\n",
    "    resized = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        image = 255 - df.loc[df.index[i]].values.reshape(137,236)\n",
    "        ymin,ymax,xmin,xmax = bbox(image[5:-5,5:-5] > 80)\n",
    "        xmin = xmin - 13 if (xmin > 13) else 0\n",
    "        ymin = ymin - 10 if (ymin > 10) else 0\n",
    "        xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "        ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "        image = image[ymin:ymax,xmin:xmax]\n",
    "        image[image < 28] = 0\n",
    "        lx, ly = xmax-xmin,ymax-ymin\n",
    "        l = max(lx,ly) + pad\n",
    "        image = np.pad(image, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "        image = cv2.resize(image,(img_size,img_size))\n",
    "        resized[df.index[i]] = image.reshape(-1)\n",
    "    resized = pd.DataFrame(resized).T\n",
    "    return resized\n",
    "def get_rand_bbox(width, height, l):\n",
    "    r_x = np.random.randint(width)\n",
    "    r_y = np.random.randint(height)\n",
    "    r_l = np.sqrt(1 - l)\n",
    "    r_w = np.int(width * r_l)\n",
    "    r_h = np.int(height * r_l)\n",
    "    return r_x, r_y, r_l, r_w, r_h\n",
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "    # custom image generator\n",
    "    def __init__(self, featurewise_center = False, samplewise_center = False, \n",
    "                 featurewise_std_normalization = False, samplewise_std_normalization = False, \n",
    "                 zca_whitening = False, zca_epsilon = 1e-06, rotation_range = 0.0, width_shift_range = 0.0, \n",
    "                 height_shift_range = 0.0, brightness_range = None, shear_range = 0.0, zoom_range = 0.0, \n",
    "                 channel_shift_range = 0.0, fill_mode = 'nearest', cval = 0.0, horizontal_flip = False, \n",
    "                 vertical_flip = False, rescale = None, preprocessing_function = None, data_format = None, validation_split = 0.0, \n",
    "                 mix_up_alpha = 0.0): # additional class argument\n",
    "    \n",
    "        # parent's constructor\n",
    "        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, \n",
    "                         zca_whitening, zca_epsilon, rotation_range, width_shift_range, height_shift_range, brightness_range, \n",
    "                         shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, \n",
    "                         preprocessing_function, data_format, validation_split)\n",
    "\n",
    "        # Mix-up\n",
    "        assert mix_up_alpha >= 0.0\n",
    "        self.mix_up_alpha = mix_up_alpha\n",
    "        \n",
    "\n",
    "\n",
    "    def mix_up(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n",
    "        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n",
    "        batch_size = X1.shape[0]\n",
    "        l = np.random.beta(self.mix_up_alpha, self.mix_up_alpha, batch_size)\n",
    "        X_l = l.reshape(batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(batch_size, 1)\n",
    "        X = X1 * X_l + X2 * (1-X_l)\n",
    "        target_dict = {}\n",
    "        i = 0\n",
    "        for output in ordered_outputs:\n",
    "            target_length = target_lengths[output]\n",
    "            target_dict[output] = y1[:, i: i + target_length] * y_l + y2[:, i: i + target_length] * (1 - y_l)\n",
    "            i += target_length\n",
    "        y = None\n",
    "        for output, target in target_dict.items():\n",
    "            if y is None:\n",
    "                y = target\n",
    "            else:\n",
    "                y = np.concatenate((y, target), axis=1)\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def flow(self,\n",
    "             x,\n",
    "             y=None,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             sample_weight=None,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png',\n",
    "             subset=None):\n",
    "        \n",
    "        # for multi-outputs\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "        \n",
    "        # parent flow\n",
    "        batches = super().flow(x, targets, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\n",
    "        \n",
    "        # custom processing\n",
    "        while True:\n",
    "            batch_x, batch_y = next(batches)\n",
    "            \n",
    "            # mixup or cutmix\n",
    "            if (self.mix_up_alpha > 0):\n",
    "                while True:\n",
    "                    batch_x_2, batch_y_2 = next(batches)\n",
    "                    m1, m2 = batch_x.shape[0], batch_x_2.shape[0]\n",
    "                    if m1 < m2:\n",
    "                        batch_x_2 = batch_x_2[:m1]\n",
    "                        batch_y_2 = batch_y_2[:m1]\n",
    "                        break\n",
    "                    elif m1 == m2:\n",
    "                        break\n",
    "                if np.random.rand() < 0.5:\n",
    "                    batch_x, batch_y = self.mix_up(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n",
    "            \n",
    "                target_dict = {}\n",
    "                i = 0\n",
    "                for output in ordered_outputs:\n",
    "                    target_length = target_lengths[output]\n",
    "                    target_dict[output] = batch_y[:, i: i + target_length]\n",
    "                    i += target_length\n",
    "                    \n",
    "                yield batch_x, target_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction_root = tf.keras.callbacks.ReduceLROnPlateau(monitor='dense_1_accuracy', \n",
    "                                                                    patience=3, \n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5, \n",
    "                                                                    min_lr=0.00001)\n",
    "learning_rate_reduction_vowel = tf.keras.callbacks.ReduceLROnPlateau(monitor='dense_2_accuracy', \n",
    "                                                                    patience=3, \n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5, \n",
    "                                                                    min_lr=0.00001)\n",
    "learning_rate_reduction_consonant = tf.keras.callbacks.ReduceLROnPlateau(monitor='dense_3_accuracy', \n",
    "                                                                    patience=3, \n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5, \n",
    "                                                                    min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 32) 9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 64, 32)   0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 128)  147584      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  147584      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 16, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  590080      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8, 8, 256)    0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 512)    1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 512)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 512)    2359808     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 512)    2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 8, 8, 512)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 512)    0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 4, 4, 512)    0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          4194816     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 512)          2048        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 168)          86184       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           5643        batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7)            3591        batch_normalization_13[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 9,206,522\n",
      "Trainable params: 9,201,082\n",
      "Non-trainable params: 5,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.2\n",
    "alpha = 0.1\n",
    "epsilon = 0.00001\n",
    "lr = 0.001\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate = lr)\n",
    "\n",
    "def build_model():\n",
    "    inputs = tf.keras.layers.Input(shape = (img_size, img_size, 1))\n",
    "\n",
    "    model = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', input_shape=(img_size, img_size, 1))(inputs) \n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(model)\n",
    "    model = tf.keras.layers.Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(model)\n",
    "    model = tf.keras.layers.Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(model)\n",
    "    model = tf.keras.layers.Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(model)\n",
    "    model = tf.keras.layers.Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    model = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(model)\n",
    "    model = tf.keras.layers.Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = tf.keras.layers.Flatten()(model)\n",
    "    model = tf.keras.layers.Dense(512)(model)\n",
    "    model = tf.keras.layers.LeakyReLU(alpha = alpha)(model)\n",
    "    dense = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum)(model)\n",
    "\n",
    "    head_root = tf.keras.layers.Dense(168, activation = 'softmax')(dense)\n",
    "    head_vowel = tf.keras.layers.Dense(11, activation = 'softmax')(dense)\n",
    "    head_consonant = tf.keras.layers.Dense(7, activation = 'softmax')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])  \n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "388/388 [==============================] - 207s 533ms/step - loss: 6.1308 - dense_1_loss: 3.7749 - dense_2_loss: 1.3049 - dense_3_loss: 1.0510 - dense_1_accuracy: 0.2500 - dense_2_accuracy: 0.7274 - dense_3_accuracy: 0.7557 - val_loss: 4.5651 - val_dense_1_loss: 2.6393 - val_dense_2_loss: 1.0446 - val_dense_3_loss: 0.8812 - val_dense_1_accuracy: 0.4732 - val_dense_2_accuracy: 0.7773 - val_dense_3_accuracy: 0.8231\n",
      "Epoch 2/15\n",
      "388/388 [==============================] - 204s 526ms/step - loss: 4.0509 - dense_1_loss: 2.2889 - dense_2_loss: 0.9973 - dense_3_loss: 0.7647 - dense_1_accuracy: 0.6700 - dense_2_accuracy: 0.8779 - dense_3_accuracy: 0.8967 - val_loss: 3.1382 - val_dense_1_loss: 1.8386 - val_dense_2_loss: 0.6999 - val_dense_3_loss: 0.5997 - val_dense_1_accuracy: 0.7018 - val_dense_2_accuracy: 0.9483 - val_dense_3_accuracy: 0.9503\n",
      "Epoch 3/15\n",
      "388/388 [==============================] - 202s 521ms/step - loss: 3.6598 - dense_1_loss: 1.9940 - dense_2_loss: 0.9425 - dense_3_loss: 0.7233 - dense_1_accuracy: 0.7783 - dense_2_accuracy: 0.9042 - dense_3_accuracy: 0.9176 - val_loss: 2.8369 - val_dense_1_loss: 1.6301 - val_dense_2_loss: 0.6446 - val_dense_3_loss: 0.5621 - val_dense_1_accuracy: 0.8131 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9622\n",
      "Epoch 4/15\n",
      "388/388 [==============================] - 201s 517ms/step - loss: 3.4221 - dense_1_loss: 1.8419 - dense_2_loss: 0.8927 - dense_3_loss: 0.6875 - dense_1_accuracy: 0.8210 - dense_2_accuracy: 0.9210 - dense_3_accuracy: 0.9337 - val_loss: 2.6577 - val_dense_1_loss: 1.4146 - val_dense_2_loss: 0.6761 - val_dense_3_loss: 0.5670 - val_dense_1_accuracy: 0.8549 - val_dense_2_accuracy: 0.9583 - val_dense_3_accuracy: 0.9662\n",
      "Epoch 5/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.3057 - dense_1_loss: 1.7609 - dense_2_loss: 0.8693 - dense_3_loss: 0.6756 - dense_1_accuracy: 0.8501 - dense_2_accuracy: 0.9299 - dense_3_accuracy: 0.9385 - val_loss: 2.5068 - val_dense_1_loss: 1.3655 - val_dense_2_loss: 0.6131 - val_dense_3_loss: 0.5281 - val_dense_1_accuracy: 0.8688 - val_dense_2_accuracy: 0.9801 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 6/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.2932 - dense_1_loss: 1.7468 - dense_2_loss: 0.8706 - dense_3_loss: 0.6757 - dense_1_accuracy: 0.8607 - dense_2_accuracy: 0.9316 - dense_3_accuracy: 0.9427 - val_loss: 2.4287 - val_dense_1_loss: 1.3095 - val_dense_2_loss: 0.5972 - val_dense_3_loss: 0.5220 - val_dense_1_accuracy: 0.8827 - val_dense_2_accuracy: 0.9861 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 7/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 3.2106 - dense_1_loss: 1.6902 - dense_2_loss: 0.8555 - dense_3_loss: 0.6648 - dense_1_accuracy: 0.8743 - dense_2_accuracy: 0.9360 - dense_3_accuracy: 0.9463 - val_loss: 2.4569 - val_dense_1_loss: 1.2914 - val_dense_2_loss: 0.6111 - val_dense_3_loss: 0.5544 - val_dense_1_accuracy: 0.8966 - val_dense_2_accuracy: 0.9781 - val_dense_3_accuracy: 0.9622\n",
      "Epoch 8/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.1594 - dense_1_loss: 1.6597 - dense_2_loss: 0.8417 - dense_3_loss: 0.6579 - dense_1_accuracy: 0.8834 - dense_2_accuracy: 0.9399 - dense_3_accuracy: 0.9474 - val_loss: 2.3493 - val_dense_1_loss: 1.2054 - val_dense_2_loss: 0.6108 - val_dense_3_loss: 0.5331 - val_dense_1_accuracy: 0.9245 - val_dense_2_accuracy: 0.9801 - val_dense_3_accuracy: 0.9702\n",
      "Epoch 9/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.1107 - dense_1_loss: 1.6281 - dense_2_loss: 0.8317 - dense_3_loss: 0.6509 - dense_1_accuracy: 0.8922 - dense_2_accuracy: 0.9422 - dense_3_accuracy: 0.9497 - val_loss: 2.2810 - val_dense_1_loss: 1.1980 - val_dense_2_loss: 0.5720 - val_dense_3_loss: 0.5109 - val_dense_1_accuracy: 0.9085 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9801\n",
      "Epoch 10/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.0337 - dense_1_loss: 1.5790 - dense_2_loss: 0.8138 - dense_3_loss: 0.6409 - dense_1_accuracy: 0.9005 - dense_2_accuracy: 0.9465 - dense_3_accuracy: 0.9529 - val_loss: 2.3086 - val_dense_1_loss: 1.2053 - val_dense_2_loss: 0.5857 - val_dense_3_loss: 0.5177 - val_dense_1_accuracy: 0.9185 - val_dense_2_accuracy: 0.9861 - val_dense_3_accuracy: 0.9742\n",
      "Epoch 11/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 3.0205 - dense_1_loss: 1.5710 - dense_2_loss: 0.8118 - dense_3_loss: 0.6376 - dense_1_accuracy: 0.9059 - dense_2_accuracy: 0.9479 - dense_3_accuracy: 0.9554 - val_loss: 2.2163 - val_dense_1_loss: 1.1445 - val_dense_2_loss: 0.5693 - val_dense_3_loss: 0.5025 - val_dense_1_accuracy: 0.9284 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9821\n",
      "Epoch 12/15\n",
      "388/388 [==============================] - 201s 517ms/step - loss: 2.9942 - dense_1_loss: 1.5530 - dense_2_loss: 0.8058 - dense_3_loss: 0.6354 - dense_1_accuracy: 0.9109 - dense_2_accuracy: 0.9489 - dense_3_accuracy: 0.9555 - val_loss: 2.2604 - val_dense_1_loss: 1.1715 - val_dense_2_loss: 0.5882 - val_dense_3_loss: 0.5007 - val_dense_1_accuracy: 0.9225 - val_dense_2_accuracy: 0.9781 - val_dense_3_accuracy: 0.9881\n",
      "Epoch 13/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.9904 - dense_1_loss: 1.5483 - dense_2_loss: 0.8073 - dense_3_loss: 0.6348 - dense_1_accuracy: 0.9126 - dense_2_accuracy: 0.9486 - dense_3_accuracy: 0.9564 - val_loss: 2.2399 - val_dense_1_loss: 1.1600 - val_dense_2_loss: 0.5791 - val_dense_3_loss: 0.5008 - val_dense_1_accuracy: 0.9284 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9801\n",
      "Epoch 14/15\n",
      "388/388 [==============================] - 200s 517ms/step - loss: 2.9022 - dense_1_loss: 1.4975 - dense_2_loss: 0.7840 - dense_3_loss: 0.6208 - dense_1_accuracy: 0.9190 - dense_2_accuracy: 0.9543 - dense_3_accuracy: 0.9592 - val_loss: 2.2100 - val_dense_1_loss: 1.1423 - val_dense_2_loss: 0.5759 - val_dense_3_loss: 0.4918 - val_dense_1_accuracy: 0.9384 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 15/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.9246 - dense_1_loss: 1.5088 - dense_2_loss: 0.7917 - dense_3_loss: 0.6241 - dense_1_accuracy: 0.9194 - dense_2_accuracy: 0.9520 - dense_3_accuracy: 0.9583 - val_loss: 2.2008 - val_dense_1_loss: 1.1356 - val_dense_2_loss: 0.5674 - val_dense_3_loss: 0.4978 - val_dense_1_accuracy: 0.9264 - val_dense_2_accuracy: 0.9861 - val_dense_3_accuracy: 0.9841\n",
      "Epoch 1/15\n",
      "388/388 [==============================] - 202s 520ms/step - loss: 3.0494 - dense_1_loss: 1.6037 - dense_2_loss: 0.8060 - dense_3_loss: 0.6397 - dense_1_accuracy: 0.8855 - dense_2_accuracy: 0.9444 - dense_3_accuracy: 0.9515 - val_loss: 2.2004 - val_dense_1_loss: 1.1071 - val_dense_2_loss: 0.5925 - val_dense_3_loss: 0.5007 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9841 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 2/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.9939 - dense_1_loss: 1.5619 - dense_2_loss: 0.7987 - dense_3_loss: 0.6332 - dense_1_accuracy: 0.8979 - dense_2_accuracy: 0.9455 - dense_3_accuracy: 0.9548 - val_loss: 2.2155 - val_dense_1_loss: 1.1155 - val_dense_2_loss: 0.5929 - val_dense_3_loss: 0.5071 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9901\n",
      "Epoch 3/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 3.0102 - dense_1_loss: 1.5680 - dense_2_loss: 0.8055 - dense_3_loss: 0.6367 - dense_1_accuracy: 0.9005 - dense_2_accuracy: 0.9464 - dense_3_accuracy: 0.9539 - val_loss: 2.2298 - val_dense_1_loss: 1.1157 - val_dense_2_loss: 0.5955 - val_dense_3_loss: 0.5186 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9722\n",
      "Epoch 4/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.9625 - dense_1_loss: 1.5392 - dense_2_loss: 0.7953 - dense_3_loss: 0.6279 - dense_1_accuracy: 0.9061 - dense_2_accuracy: 0.9517 - dense_3_accuracy: 0.9575 - val_loss: 2.2559 - val_dense_1_loss: 1.1190 - val_dense_2_loss: 0.5982 - val_dense_3_loss: 0.5387 - val_dense_1_accuracy: 0.9264 - val_dense_2_accuracy: 0.9801 - val_dense_3_accuracy: 0.9742\n",
      "Epoch 5/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.9428 - dense_1_loss: 1.5253 - dense_2_loss: 0.7903 - dense_3_loss: 0.6271 - dense_1_accuracy: 0.9112 - dense_2_accuracy: 0.9524 - dense_3_accuracy: 0.9590 - val_loss: 2.2194 - val_dense_1_loss: 1.1106 - val_dense_2_loss: 0.5927 - val_dense_3_loss: 0.5162 - val_dense_1_accuracy: 0.9344 - val_dense_2_accuracy: 0.9781 - val_dense_3_accuracy: 0.9781\n",
      "Epoch 6/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.8957 - dense_1_loss: 1.4952 - dense_2_loss: 0.7803 - dense_3_loss: 0.6201 - dense_1_accuracy: 0.9178 - dense_2_accuracy: 0.9528 - dense_3_accuracy: 0.9595 - val_loss: 2.1553 - val_dense_1_loss: 1.0669 - val_dense_2_loss: 0.5848 - val_dense_3_loss: 0.5037 - val_dense_1_accuracy: 0.9463 - val_dense_2_accuracy: 0.9801 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 7/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.8919 - dense_1_loss: 1.4940 - dense_2_loss: 0.7793 - dense_3_loss: 0.6187 - dense_1_accuracy: 0.9202 - dense_2_accuracy: 0.9548 - dense_3_accuracy: 0.9609 - val_loss: 2.1582 - val_dense_1_loss: 1.0774 - val_dense_2_loss: 0.5869 - val_dense_3_loss: 0.4939 - val_dense_1_accuracy: 0.9384 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9881\n",
      "Epoch 8/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.8896 - dense_1_loss: 1.4891 - dense_2_loss: 0.7808 - dense_3_loss: 0.6198 - dense_1_accuracy: 0.9226 - dense_2_accuracy: 0.9536 - dense_3_accuracy: 0.9616 - val_loss: 2.2148 - val_dense_1_loss: 1.1207 - val_dense_2_loss: 0.5922 - val_dense_3_loss: 0.5018 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9841 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 9/15\n",
      "388/388 [==============================] - 199s 514ms/step - loss: 2.8614 - dense_1_loss: 1.4729 - dense_2_loss: 0.7741 - dense_3_loss: 0.6144 - dense_1_accuracy: 0.9253 - dense_2_accuracy: 0.9560 - dense_3_accuracy: 0.9626 - val_loss: 2.1810 - val_dense_1_loss: 1.0925 - val_dense_2_loss: 0.5873 - val_dense_3_loss: 0.5012 - val_dense_1_accuracy: 0.9245 - val_dense_2_accuracy: 0.9781 - val_dense_3_accuracy: 0.9801\n",
      "Epoch 10/15\n",
      "388/388 [==============================] - 199s 514ms/step - loss: 2.8518 - dense_1_loss: 1.4659 - dense_2_loss: 0.7725 - dense_3_loss: 0.6134 - dense_1_accuracy: 0.9282 - dense_2_accuracy: 0.9551 - dense_3_accuracy: 0.9633 - val_loss: 2.1703 - val_dense_1_loss: 1.0800 - val_dense_2_loss: 0.5868 - val_dense_3_loss: 0.5035 - val_dense_1_accuracy: 0.9423 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9781\n",
      "Epoch 11/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.7919 - dense_1_loss: 1.4290 - dense_2_loss: 0.7570 - dense_3_loss: 0.6059 - dense_1_accuracy: 0.9320 - dense_2_accuracy: 0.9582 - dense_3_accuracy: 0.9638 - val_loss: 2.1924 - val_dense_1_loss: 1.1034 - val_dense_2_loss: 0.5863 - val_dense_3_loss: 0.5027 - val_dense_1_accuracy: 0.9443 - val_dense_2_accuracy: 0.9841 - val_dense_3_accuracy: 0.9781\n",
      "Epoch 12/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.8224 - dense_1_loss: 1.4481 - dense_2_loss: 0.7653 - dense_3_loss: 0.6091 - dense_1_accuracy: 0.9307 - dense_2_accuracy: 0.9564 - dense_3_accuracy: 0.9636 - val_loss: 2.1773 - val_dense_1_loss: 1.0866 - val_dense_2_loss: 0.5885 - val_dense_3_loss: 0.5022 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9841\n",
      "Epoch 13/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.8331 - dense_1_loss: 1.4537 - dense_2_loss: 0.7695 - dense_3_loss: 0.6099 - dense_1_accuracy: 0.9317 - dense_2_accuracy: 0.9567 - dense_3_accuracy: 0.9631 - val_loss: 2.1589 - val_dense_1_loss: 1.0661 - val_dense_2_loss: 0.5823 - val_dense_3_loss: 0.5106 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9761 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 14/15\n",
      "388/388 [==============================] - 200s 517ms/step - loss: 2.7831 - dense_1_loss: 1.4237 - dense_2_loss: 0.7555 - dense_3_loss: 0.6039 - dense_1_accuracy: 0.9353 - dense_2_accuracy: 0.9602 - dense_3_accuracy: 0.9654 - val_loss: 2.1363 - val_dense_1_loss: 1.0539 - val_dense_2_loss: 0.5839 - val_dense_3_loss: 0.4985 - val_dense_1_accuracy: 0.9443 - val_dense_2_accuracy: 0.9821 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 15/15\n",
      "388/388 [==============================] - 201s 517ms/step - loss: 2.7879 - dense_1_loss: 1.4265 - dense_2_loss: 0.7578 - dense_3_loss: 0.6035 - dense_1_accuracy: 0.9351 - dense_2_accuracy: 0.9579 - dense_3_accuracy: 0.9647 - val_loss: 2.1580 - val_dense_1_loss: 1.0729 - val_dense_2_loss: 0.5869 - val_dense_3_loss: 0.4982 - val_dense_1_accuracy: 0.9324 - val_dense_2_accuracy: 0.9801 - val_dense_3_accuracy: 0.9821\n",
      "Epoch 1/15\n",
      "388/388 [==============================] - 203s 524ms/step - loss: 2.9433 - dense_1_loss: 1.5373 - dense_2_loss: 0.7838 - dense_3_loss: 0.6222 - dense_1_accuracy: 0.8977 - dense_2_accuracy: 0.9469 - dense_3_accuracy: 0.9566 - val_loss: 2.1735 - val_dense_1_loss: 1.0754 - val_dense_2_loss: 0.6000 - val_dense_3_loss: 0.4981 - val_dense_1_accuracy: 0.9563 - val_dense_2_accuracy: 0.9702 - val_dense_3_accuracy: 0.9801\n",
      "Epoch 2/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.9083 - dense_1_loss: 1.5093 - dense_2_loss: 0.7791 - dense_3_loss: 0.6200 - dense_1_accuracy: 0.9081 - dense_2_accuracy: 0.9501 - dense_3_accuracy: 0.9585 - val_loss: 2.1901 - val_dense_1_loss: 1.0804 - val_dense_2_loss: 0.6037 - val_dense_3_loss: 0.5060 - val_dense_1_accuracy: 0.9523 - val_dense_2_accuracy: 0.9662 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 3/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.8682 - dense_1_loss: 1.4839 - dense_2_loss: 0.7685 - dense_3_loss: 0.6158 - dense_1_accuracy: 0.9148 - dense_2_accuracy: 0.9553 - dense_3_accuracy: 0.9588 - val_loss: 2.1531 - val_dense_1_loss: 1.0629 - val_dense_2_loss: 0.5951 - val_dense_3_loss: 0.4951 - val_dense_1_accuracy: 0.9503 - val_dense_2_accuracy: 0.9742 - val_dense_3_accuracy: 0.9781\n",
      "Epoch 4/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.8830 - dense_1_loss: 1.4898 - dense_2_loss: 0.7770 - dense_3_loss: 0.6162 - dense_1_accuracy: 0.9155 - dense_2_accuracy: 0.9531 - dense_3_accuracy: 0.9612 - val_loss: 2.1748 - val_dense_1_loss: 1.0845 - val_dense_2_loss: 0.5961 - val_dense_3_loss: 0.4942 - val_dense_1_accuracy: 0.9443 - val_dense_2_accuracy: 0.9742 - val_dense_3_accuracy: 0.9801\n",
      "Epoch 5/15\n",
      "388/388 [==============================] - 204s 526ms/step - loss: 2.8218 - dense_1_loss: 1.4515 - dense_2_loss: 0.7625 - dense_3_loss: 0.6079 - dense_1_accuracy: 0.9224 - dense_2_accuracy: 0.9566 - dense_3_accuracy: 0.9621 - val_loss: 2.1273 - val_dense_1_loss: 1.0460 - val_dense_2_loss: 0.5966 - val_dense_3_loss: 0.4847 - val_dense_1_accuracy: 0.9523 - val_dense_2_accuracy: 0.9722 - val_dense_3_accuracy: 0.9841\n",
      "Epoch 6/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.8262 - dense_1_loss: 1.4530 - dense_2_loss: 0.7649 - dense_3_loss: 0.6084 - dense_1_accuracy: 0.9247 - dense_2_accuracy: 0.9548 - dense_3_accuracy: 0.9629 - val_loss: 2.1595 - val_dense_1_loss: 1.0643 - val_dense_2_loss: 0.6037 - val_dense_3_loss: 0.4915 - val_dense_1_accuracy: 0.9543 - val_dense_2_accuracy: 0.9662 - val_dense_3_accuracy: 0.9841\n",
      "Epoch 7/15\n",
      "388/388 [==============================] - 200s 514ms/step - loss: 2.7841 - dense_1_loss: 1.4262 - dense_2_loss: 0.7544 - dense_3_loss: 0.6035 - dense_1_accuracy: 0.9303 - dense_2_accuracy: 0.9579 - dense_3_accuracy: 0.9650 - val_loss: 2.1532 - val_dense_1_loss: 1.0669 - val_dense_2_loss: 0.6001 - val_dense_3_loss: 0.4861 - val_dense_1_accuracy: 0.9483 - val_dense_2_accuracy: 0.9702 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 8/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.8447 - dense_1_loss: 1.4598 - dense_2_loss: 0.7724 - dense_3_loss: 0.6125 - dense_1_accuracy: 0.9281 - dense_2_accuracy: 0.9547 - dense_3_accuracy: 0.9631 - val_loss: 2.1200 - val_dense_1_loss: 1.0362 - val_dense_2_loss: 0.5932 - val_dense_3_loss: 0.4905 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9761 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 9/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.7928 - dense_1_loss: 1.4310 - dense_2_loss: 0.7582 - dense_3_loss: 0.6036 - dense_1_accuracy: 0.9297 - dense_2_accuracy: 0.9577 - dense_3_accuracy: 0.9646 - val_loss: 2.1380 - val_dense_1_loss: 1.0548 - val_dense_2_loss: 0.5899 - val_dense_3_loss: 0.4932 - val_dense_1_accuracy: 0.9543 - val_dense_2_accuracy: 0.9722 - val_dense_3_accuracy: 0.9821\n",
      "Epoch 10/15\n",
      "387/388 [============================>.] - ETA: 0s - loss: 2.8586 - dense_1_loss: 1.4710 - dense_2_loss: 0.7755 - dense_3_loss: 0.6121 - dense_1_accuracy: 0.9295 - dense_2_accuracy: 0.9552 - dense_3_accuracy: 0.9639\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "388/388 [==============================] - 199s 512ms/step - loss: 2.8606 - dense_1_loss: 1.4721 - dense_2_loss: 0.7760 - dense_3_loss: 0.6125 - dense_1_accuracy: 0.9293 - dense_2_accuracy: 0.9550 - dense_3_accuracy: 0.9638 - val_loss: 2.1486 - val_dense_1_loss: 1.0528 - val_dense_2_loss: 0.5935 - val_dense_3_loss: 0.5024 - val_dense_1_accuracy: 0.9662 - val_dense_2_accuracy: 0.9761 - val_dense_3_accuracy: 0.9761\n",
      "Epoch 11/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.7382 - dense_1_loss: 1.3927 - dense_2_loss: 0.7498 - dense_3_loss: 0.5957 - dense_1_accuracy: 0.9419 - dense_2_accuracy: 0.9614 - dense_3_accuracy: 0.9686 - val_loss: 2.0907 - val_dense_1_loss: 1.0207 - val_dense_2_loss: 0.5833 - val_dense_3_loss: 0.4867 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9761 - val_dense_3_accuracy: 0.9841\n",
      "Epoch 12/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.7356 - dense_1_loss: 1.3920 - dense_2_loss: 0.7501 - dense_3_loss: 0.5934 - dense_1_accuracy: 0.9448 - dense_2_accuracy: 0.9618 - dense_3_accuracy: 0.9709 - val_loss: 2.0846 - val_dense_1_loss: 1.0175 - val_dense_2_loss: 0.5832 - val_dense_3_loss: 0.4838 - val_dense_1_accuracy: 0.9662 - val_dense_2_accuracy: 0.9722 - val_dense_3_accuracy: 0.9861\n",
      "Epoch 13/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.7229 - dense_1_loss: 1.3818 - dense_2_loss: 0.7471 - dense_3_loss: 0.5940 - dense_1_accuracy: 0.9467 - dense_2_accuracy: 0.9622 - dense_3_accuracy: 0.9693 - val_loss: 2.0817 - val_dense_1_loss: 1.0130 - val_dense_2_loss: 0.5839 - val_dense_3_loss: 0.4848 - val_dense_1_accuracy: 0.9662 - val_dense_2_accuracy: 0.9722 - val_dense_3_accuracy: 0.9821\n",
      "Epoch 14/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.6850 - dense_1_loss: 1.3595 - dense_2_loss: 0.7376 - dense_3_loss: 0.5879 - dense_1_accuracy: 0.9469 - dense_2_accuracy: 0.9640 - dense_3_accuracy: 0.9710 - val_loss: 2.0832 - val_dense_1_loss: 1.0130 - val_dense_2_loss: 0.5839 - val_dense_3_loss: 0.4863 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9742 - val_dense_3_accuracy: 0.9821\n",
      "Epoch 15/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.6587 - dense_1_loss: 1.3439 - dense_2_loss: 0.7304 - dense_3_loss: 0.5844 - dense_1_accuracy: 0.9489 - dense_2_accuracy: 0.9650 - dense_3_accuracy: 0.9718 - val_loss: 2.0802 - val_dense_1_loss: 1.0111 - val_dense_2_loss: 0.5855 - val_dense_3_loss: 0.4836 - val_dense_1_accuracy: 0.9682 - val_dense_2_accuracy: 0.9702 - val_dense_3_accuracy: 0.9881\n",
      "Epoch 1/15\n",
      "388/388 [==============================] - 203s 522ms/step - loss: 2.8150 - dense_1_loss: 1.4536 - dense_2_loss: 0.7580 - dense_3_loss: 0.6034 - dense_1_accuracy: 0.9146 - dense_2_accuracy: 0.9547 - dense_3_accuracy: 0.9625 - val_loss: 2.0330 - val_dense_1_loss: 0.9905 - val_dense_2_loss: 0.5665 - val_dense_3_loss: 0.4760 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 2/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.8401 - dense_1_loss: 1.4656 - dense_2_loss: 0.7670 - dense_3_loss: 0.6075 - dense_1_accuracy: 0.9151 - dense_2_accuracy: 0.9538 - dense_3_accuracy: 0.9625 - val_loss: 2.0341 - val_dense_1_loss: 0.9936 - val_dense_2_loss: 0.5658 - val_dense_3_loss: 0.4747 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 3/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.7697 - dense_1_loss: 1.4240 - dense_2_loss: 0.7485 - dense_3_loss: 0.5972 - dense_1_accuracy: 0.9194 - dense_2_accuracy: 0.9564 - dense_3_accuracy: 0.9631 - val_loss: 2.0264 - val_dense_1_loss: 0.9901 - val_dense_2_loss: 0.5643 - val_dense_3_loss: 0.4720 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 4/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.8040 - dense_1_loss: 1.4438 - dense_2_loss: 0.7578 - dense_3_loss: 0.6025 - dense_1_accuracy: 0.9187 - dense_2_accuracy: 0.9557 - dense_3_accuracy: 0.9631 - val_loss: 2.0277 - val_dense_1_loss: 0.9924 - val_dense_2_loss: 0.5629 - val_dense_3_loss: 0.4724 - val_dense_1_accuracy: 0.9602 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 5/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.8122 - dense_1_loss: 1.4464 - dense_2_loss: 0.7619 - dense_3_loss: 0.6040 - dense_1_accuracy: 0.9205 - dense_2_accuracy: 0.9555 - dense_3_accuracy: 0.9644 - val_loss: 2.0190 - val_dense_1_loss: 0.9857 - val_dense_2_loss: 0.5633 - val_dense_3_loss: 0.4701 - val_dense_1_accuracy: 0.9642 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9940\n",
      "Epoch 6/15\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.6955 - dense_1_loss: 1.3798 - dense_2_loss: 0.7298 - dense_3_loss: 0.5859 - dense_1_accuracy: 0.9262 - dense_2_accuracy: 0.9605 - dense_3_accuracy: 0.9678 - val_loss: 2.0222 - val_dense_1_loss: 0.9857 - val_dense_2_loss: 0.5639 - val_dense_3_loss: 0.4727 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 7/15\n",
      "388/388 [==============================] - 201s 517ms/step - loss: 2.6909 - dense_1_loss: 1.3758 - dense_2_loss: 0.7290 - dense_3_loss: 0.5862 - dense_1_accuracy: 0.9286 - dense_2_accuracy: 0.9610 - dense_3_accuracy: 0.9680 - val_loss: 2.0211 - val_dense_1_loss: 0.9854 - val_dense_2_loss: 0.5633 - val_dense_3_loss: 0.4724 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9861 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 8/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.7378 - dense_1_loss: 1.3997 - dense_2_loss: 0.7444 - dense_3_loss: 0.5936 - dense_1_accuracy: 0.9287 - dense_2_accuracy: 0.9592 - dense_3_accuracy: 0.9661 - val_loss: 2.0215 - val_dense_1_loss: 0.9833 - val_dense_2_loss: 0.5643 - val_dense_3_loss: 0.4739 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9861 - val_dense_3_accuracy: 0.9901\n",
      "Epoch 9/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.7536 - dense_1_loss: 1.4097 - dense_2_loss: 0.7476 - dense_3_loss: 0.5964 - dense_1_accuracy: 0.9272 - dense_2_accuracy: 0.9586 - dense_3_accuracy: 0.9658 - val_loss: 2.0189 - val_dense_1_loss: 0.9829 - val_dense_2_loss: 0.5629 - val_dense_3_loss: 0.4731 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 10/15\n",
      "387/388 [============================>.] - ETA: 0s - loss: 2.7517 - dense_1_loss: 1.4063 - dense_2_loss: 0.7478 - dense_3_loss: 0.5977 - dense_1_accuracy: 0.9287 - dense_2_accuracy: 0.9590 - dense_3_accuracy: 0.9658\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.7535 - dense_1_loss: 1.4073 - dense_2_loss: 0.7483 - dense_3_loss: 0.5978 - dense_1_accuracy: 0.9286 - dense_2_accuracy: 0.9589 - dense_3_accuracy: 0.9658 - val_loss: 2.0221 - val_dense_1_loss: 0.9862 - val_dense_2_loss: 0.5644 - val_dense_3_loss: 0.4715 - val_dense_1_accuracy: 0.9543 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 11/15\n",
      "388/388 [==============================] - 199s 513ms/step - loss: 2.7572 - dense_1_loss: 1.4101 - dense_2_loss: 0.7507 - dense_3_loss: 0.5963 - dense_1_accuracy: 0.9294 - dense_2_accuracy: 0.9584 - dense_3_accuracy: 0.9669 - val_loss: 2.0185 - val_dense_1_loss: 0.9852 - val_dense_2_loss: 0.5625 - val_dense_3_loss: 0.4708 - val_dense_1_accuracy: 0.9583 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 12/15\n",
      "388/388 [==============================] - 199s 514ms/step - loss: 2.7535 - dense_1_loss: 1.4087 - dense_2_loss: 0.7485 - dense_3_loss: 0.5963 - dense_1_accuracy: 0.9303 - dense_2_accuracy: 0.9598 - dense_3_accuracy: 0.9669 - val_loss: 2.0204 - val_dense_1_loss: 0.9867 - val_dense_2_loss: 0.5626 - val_dense_3_loss: 0.4712 - val_dense_1_accuracy: 0.9602 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "387/388 [============================>.] - ETA: 0s - loss: 2.6917 - dense_1_loss: 1.3721 - dense_2_loss: 0.7320 - dense_3_loss: 0.5876 - dense_1_accuracy: 0.9326 - dense_2_accuracy: 0.9606 - dense_3_accuracy: 0.9679\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "388/388 [==============================] - 200s 515ms/step - loss: 2.6932 - dense_1_loss: 1.3730 - dense_2_loss: 0.7323 - dense_3_loss: 0.5878 - dense_1_accuracy: 0.9325 - dense_2_accuracy: 0.9605 - dense_3_accuracy: 0.9678 - val_loss: 2.0173 - val_dense_1_loss: 0.9843 - val_dense_2_loss: 0.5621 - val_dense_3_loss: 0.4708 - val_dense_1_accuracy: 0.9622 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 14/15\n",
      "388/388 [==============================] - 200s 516ms/step - loss: 2.6792 - dense_1_loss: 1.3651 - dense_2_loss: 0.7294 - dense_3_loss: 0.5847 - dense_1_accuracy: 0.9342 - dense_2_accuracy: 0.9627 - dense_3_accuracy: 0.9674 - val_loss: 2.0176 - val_dense_1_loss: 0.9843 - val_dense_2_loss: 0.5625 - val_dense_3_loss: 0.4708 - val_dense_1_accuracy: 0.9602 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n",
      "Epoch 15/15\n",
      "388/388 [==============================] - 200s 514ms/step - loss: 2.7404 - dense_1_loss: 1.3992 - dense_2_loss: 0.7461 - dense_3_loss: 0.5952 - dense_1_accuracy: 0.9310 - dense_2_accuracy: 0.9600 - dense_3_accuracy: 0.9678 - val_loss: 2.0226 - val_dense_1_loss: 0.9869 - val_dense_2_loss: 0.5631 - val_dense_3_loss: 0.4726 - val_dense_1_accuracy: 0.9602 - val_dense_2_accuracy: 0.9881 - val_dense_3_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    train_df = pd.merge(read_data(i), train_df_, on='image_id').drop(['image_id','grapheme'], axis=1)\n",
    "\n",
    "    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n",
    "    X_train = np.divide(resize(X_train), 255.0, dtype = 'float16')\n",
    "\n",
    "    X_train = X_train.values.reshape(-1, img_size, img_size, 1)\n",
    "\n",
    "    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n",
    "    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n",
    "    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n",
    "\n",
    "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.01, random_state = 111)\n",
    "\n",
    "\n",
    "    del train_df\n",
    "    del X_train\n",
    "    del Y_train_root, Y_train_vowel, Y_train_consonant\n",
    "\n",
    "    datagen = MultiOutputDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180, was 8)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        mix_up_alpha = 0.4)\n",
    "        #cutmix_alpha = 0.4) \n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, {'dense_1': y_train_root, 'dense_2': y_train_vowel, 'dense_3': y_train_consonant}, batch_size=batch_size),\n",
    "                        epochs = 15, \n",
    "                        validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
    "                        callbacks = [learning_rate_reduction_root,learning_rate_reduction_vowel, learning_rate_reduction_consonant],\n",
    "                        steps_per_epoch = len(x_train) // batch_size\n",
    "                       )\n",
    "\n",
    "    del x_train, x_test\n",
    "    del y_train_root, y_test_root\n",
    "    del y_train_vowel, y_test_vowel\n",
    "    del y_train_consonant, y_test_consonant\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('128_mixup_float16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
