{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, MaxPool2D, LeakyReLU, Conv2D, Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'C:/SSD_Projects/Bengali_git/input/'\n",
    "train_df_ = pd.read_csv(os.path.join(input_path, 'train.csv'))\n",
    "train_df_ = np.array_split(train_df_,4)\n",
    "img_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(nf):\n",
    "    nf=int(nf)\n",
    "    train_df = np.load(os.path.join(input_path,f'train_image_data_{nf}.npy'))\n",
    "    return train_df\n",
    "def get_rand_bbox(width, height, l):\n",
    "    r_x = np.random.randint(width)\n",
    "    r_y = np.random.randint(height)\n",
    "    r_l = np.sqrt(1 - l)\n",
    "    r_w = np.int(width * r_l)\n",
    "    r_h = np.int(height * r_l)\n",
    "    return r_x, r_y, r_l, r_w, r_h\n",
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "    # custom image generator\n",
    "    def __init__(self, featurewise_center = False, samplewise_center = False, \n",
    "                 featurewise_std_normalization = False, samplewise_std_normalization = False, \n",
    "                 zca_whitening = False, zca_epsilon = 1e-06, rotation_range = 0.0, width_shift_range = 0.0, \n",
    "                 height_shift_range = 0.0, brightness_range = None, shear_range = 0.0, zoom_range = 0.0, \n",
    "                 channel_shift_range = 0.0, fill_mode = 'nearest', cval = 0.0, horizontal_flip = False, \n",
    "                 vertical_flip = False, rescale = None, preprocessing_function = None, data_format = None, validation_split = 0.0, \n",
    "                 mix_up_alpha = 0.0): # additional class argument\n",
    "    \n",
    "\n",
    "        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, \n",
    "                         zca_whitening, zca_epsilon, rotation_range, width_shift_range, height_shift_range, brightness_range, \n",
    "                         shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, \n",
    "                         preprocessing_function, data_format, validation_split)\n",
    "\n",
    "        # Mix-up\n",
    "        assert mix_up_alpha >= 0.0\n",
    "        self.mix_up_alpha = mix_up_alpha\n",
    "        \n",
    "\n",
    "\n",
    "    def mix_up(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n",
    "        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n",
    "        batch_size = X1.shape[0]\n",
    "        l = np.random.beta(self.mix_up_alpha, self.mix_up_alpha, batch_size)\n",
    "        X_l = l.reshape(batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(batch_size, 1)\n",
    "        X = X1 * X_l + X2 * (1-X_l)\n",
    "        target_dict = {}\n",
    "        i = 0\n",
    "        for output in ordered_outputs:\n",
    "            target_length = target_lengths[output]\n",
    "            target_dict[output] = y1[:, i: i + target_length] * y_l + y2[:, i: i + target_length] * (1 - y_l)\n",
    "            i += target_length\n",
    "        y = None\n",
    "        for output, target in target_dict.items():\n",
    "            if y is None:\n",
    "                y = target\n",
    "            else:\n",
    "                y = np.concatenate((y, target), axis=1)\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def flow(self,\n",
    "             x,\n",
    "             y=None,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             sample_weight=None,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png',\n",
    "             subset=None):\n",
    "        \n",
    "        # for multi-outputs\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "        \n",
    "        # parent flow\n",
    "        batches = super().flow(x, targets, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\n",
    "        \n",
    "        # custom processing\n",
    "        while True:\n",
    "            batch_x, batch_y = next(batches)\n",
    "            \n",
    "            # mixup or cutmix\n",
    "            if (self.mix_up_alpha > 0):\n",
    "                while True:\n",
    "                    batch_x_2, batch_y_2 = next(batches)\n",
    "                    m1, m2 = batch_x.shape[0], batch_x_2.shape[0]\n",
    "                    if m1 < m2:\n",
    "                        batch_x_2 = batch_x_2[:m1]\n",
    "                        batch_y_2 = batch_y_2[:m1]\n",
    "                        break\n",
    "                    elif m1 == m2:\n",
    "                        break\n",
    "                if np.random.rand() < 0.5:\n",
    "                    batch_x, batch_y = self.mix_up(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n",
    "            \n",
    "                target_dict = {}\n",
    "                i = 0\n",
    "                for output in ordered_outputs:\n",
    "                    target_length = target_lengths[output]\n",
    "                    target_dict[output] = batch_y[:, i: i + target_length]\n",
    "                    i += target_length\n",
    "                    \n",
    "                yield batch_x, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.2\n",
    "alpha = 0.1\n",
    "epsilon = 0.00001\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate = lr)\n",
    "\n",
    "def build_model():\n",
    "    inputs = Input(shape = (img_size, img_size, 1))\n",
    "\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', input_shape=(img_size, img_size, 1))(inputs)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = Conv2D(filters=512, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    model = Conv2D(filters=512, kernel_size=(3, 3), padding='SAME')(model)\n",
    "    model = BatchNormalization(axis=-1, momentum=momentum, epsilon = epsilon, gamma_initializer=\"uniform\")(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    \n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Dropout(rate = dropout_rate)(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(512)(model)\n",
    "    model = LeakyReLU(alpha = alpha)(model)\n",
    "    dense = BatchNormalization(axis=-1, momentum=momentum)(model)\n",
    "\n",
    "    head_root = Dense(168, activation = 'softmax')(dense)\n",
    "    head_vowel = Dense(11, activation = 'softmax')(dense)\n",
    "    head_consonant = Dense(7, activation = 'softmax')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])  \n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=opt, loss=CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_1_accuracy',\n",
    "                                                                    patience=3,\n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5,\n",
    "                                                                    min_lr=0.00001)\n",
    "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_2_accuracy',\n",
    "                                                                    patience=3,\n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5,\n",
    "                                                                    min_lr=0.00001)\n",
    "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_3_accuracy',\n",
    "                                                                    patience=3,\n",
    "                                                                    verbose=1,\n",
    "                                                                    factor=0.5,\n",
    "                                                                    min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    x_df = read_data(i)\n",
    "\n",
    "    y_df_root = pd.get_dummies(train_df_[i]['grapheme_root']).values\n",
    "    y_df_vowel = pd.get_dummies(train_df_[i]['vowel_diacritic']).values\n",
    "    y_df_consonant = pd.get_dummies(train_df_[i]['consonant_diacritic']).values\n",
    "\n",
    "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(x_df, y_df_root, y_df_vowel, y_df_consonant, test_size=0.01, random_state = 111)\n",
    "\n",
    "    del x_df\n",
    "    del y_df_root, y_df_vowel, y_df_consonant\n",
    "\n",
    "    datagen = MultiOutputDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180, was 8)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        mix_up_alpha = 0.4)\n",
    "        #cutmix_alpha = 0.4) \n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, {'dense_1': y_train_root, 'dense_2': y_train_vowel, 'dense_3': y_train_consonant}, batch_size=batch_size),\n",
    "                        epochs = 15, \n",
    "                        validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
    "                        callbacks = [learning_rate_reduction_root,learning_rate_reduction_vowel, learning_rate_reduction_consonant],\n",
    "                        steps_per_epoch = len(x_train) // batch_size\n",
    "                       )\n",
    "\n",
    "    del x_train, x_test\n",
    "    del y_train_root, y_test_root\n",
    "    del y_train_vowel, y_test_vowel\n",
    "    del y_train_consonant, y_test_consonant\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}